{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d2702c43",
   "metadata": {},
   "source": [
    "##### Name: Anmol Chandra Singh\n",
    "##### Github Username: anmol-singh-usc\n",
    "##### USCID: 2575049221"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96364637",
   "metadata": {},
   "source": [
    "###### All imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "80a3213a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import re\n",
    "from scipy.stats import bootstrap\n",
    "from tabulate import tabulate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c1c85bd",
   "metadata": {},
   "source": [
    "###### a) Download the AReM data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97214fb4",
   "metadata": {},
   "source": [
    "###### Note on data cleaning:\n",
    "\n",
    "- dataset 9 and 14 in the cycling folder had and erroneous comma at the end of the file\n",
    "- dataset 4 in the bending2 folder had data in space separated format instead of comma separated format\n",
    "\n",
    "All of this was cleaned manually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2aa551a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_csv(path: str):\n",
    "\n",
    "    files = [f for f in glob.glob(path)]\n",
    "    df = []\n",
    "    for f in files:\n",
    "        try:\n",
    "            data = pd.read_csv(f, skiprows=4)    \n",
    "        except Exception as e:\n",
    "            print(f'Error: {e} in file: {f}')\n",
    "        data[\"target\"] = f.split(\"/\")[-2]\n",
    "        data[\"dataset_idx\"] = re.findall(r'\\d+', f.split(\"/\")[-1])[0]\n",
    "        df.append(data)\n",
    "    return pd.concat(df)\n",
    "\n",
    "root = \"../data/AReM/\"\n",
    "\n",
    "dirs = [\n",
    "    \"bending1\",\n",
    "    \"bending2\",\n",
    "    \"cycling\", \n",
    "    \"lying\",\n",
    "    \"sitting\",\n",
    "    \"standing\", \n",
    "    \"walking\"\n",
    "]\n",
    "\n",
    "\n",
    "csv = []\n",
    "\n",
    "for directory in dirs:\n",
    "    dir_data = read_csv(f'{root}{directory}/*')\n",
    "    csv.append(dir_data)\n",
    "\n",
    "[bending_1, bending_2, cycling, lying, sitting, standing, walking] = csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3695a5b1",
   "metadata": {},
   "source": [
    "###### b) Keep datasets 1 and 2 in folders bending1 and bending 2, as well as datasets 1, 2, and 3 in other folders as test data and other datasets as train data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "08a09e0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_12 = [\"1\", \"2\"]\n",
    "d_123 = d_12 + [\"3\"]\n",
    "\n",
    "bending_1_train = bending_1[~bending_1[\"dataset_idx\"].isin(d_12)]\n",
    "bending_1_test = bending_1[bending_1[\"dataset_idx\"].isin(d_12)]\n",
    "\n",
    "bending_2_train = bending_2[~bending_2[\"dataset_idx\"].isin(d_12)]\n",
    "bending_2_test = bending_2[bending_2[\"dataset_idx\"].isin(d_12)]\n",
    "\n",
    "cycling_train = cycling[~cycling[\"dataset_idx\"].isin(d_123)]\n",
    "cycling_test = cycling[cycling[\"dataset_idx\"].isin(d_123)]\n",
    "\n",
    "lying_train = lying[~lying[\"dataset_idx\"].isin(d_123)]\n",
    "lying_test = lying[lying[\"dataset_idx\"].isin(d_123)]\n",
    "\n",
    "sitting_train = sitting[~sitting[\"dataset_idx\"].isin(d_123)]\n",
    "sitting_test = sitting[sitting[\"dataset_idx\"].isin(d_123)]\n",
    "\n",
    "standing_train = standing[~standing[\"dataset_idx\"].isin(d_123)]\n",
    "standing_test = standing[standing[\"dataset_idx\"].isin(d_123)]\n",
    "\n",
    "walking_train = walking[~walking[\"dataset_idx\"].isin(d_123)]\n",
    "walking_test = walking[walking[\"dataset_idx\"].isin(d_123)]\n",
    "\n",
    "train_data = pd.concat([\n",
    "    bending_1_train, \n",
    "    bending_2_train, \n",
    "    cycling_train,\n",
    "    lying_train, \n",
    "    sitting_train, \n",
    "    standing_train, \n",
    "    walking_train\n",
    "])\n",
    "\n",
    "train_data.rename(columns={\"# Columns: time\": \"time\"}, inplace=True)\n",
    "\n",
    "train_data.reset_index(drop=True, inplace=True)\n",
    "\n",
    "test_data = pd.concat([\n",
    "    bending_1_test,\n",
    "    bending_2_test,\n",
    "    cycling_test,\n",
    "    lying_test,\n",
    "    sitting_test,\n",
    "    standing_test,\n",
    "    walking_test\n",
    "])\n",
    "\n",
    "test_data.rename(columns={\"# Columns: time\": \"time\"}, inplace=True)\n",
    "\n",
    "test_data.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "67f9f03e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------- TRAINING DATA -------------------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time</th>\n",
       "      <th>avg_rss12</th>\n",
       "      <th>var_rss12</th>\n",
       "      <th>avg_rss13</th>\n",
       "      <th>var_rss13</th>\n",
       "      <th>avg_rss23</th>\n",
       "      <th>var_rss23</th>\n",
       "      <th>target</th>\n",
       "      <th>dataset_idx</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>42.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>18.50</td>\n",
       "      <td>0.50</td>\n",
       "      <td>12.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>bending1</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>250</td>\n",
       "      <td>42.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>18.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>11.33</td>\n",
       "      <td>0.94</td>\n",
       "      <td>bending1</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>500</td>\n",
       "      <td>42.75</td>\n",
       "      <td>0.43</td>\n",
       "      <td>16.75</td>\n",
       "      <td>1.79</td>\n",
       "      <td>18.25</td>\n",
       "      <td>0.43</td>\n",
       "      <td>bending1</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>750</td>\n",
       "      <td>42.50</td>\n",
       "      <td>0.50</td>\n",
       "      <td>16.75</td>\n",
       "      <td>0.83</td>\n",
       "      <td>19.00</td>\n",
       "      <td>1.22</td>\n",
       "      <td>bending1</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1000</td>\n",
       "      <td>43.00</td>\n",
       "      <td>0.82</td>\n",
       "      <td>16.25</td>\n",
       "      <td>0.83</td>\n",
       "      <td>18.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>bending1</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33114</th>\n",
       "      <td>118750</td>\n",
       "      <td>31.50</td>\n",
       "      <td>1.66</td>\n",
       "      <td>12.50</td>\n",
       "      <td>3.20</td>\n",
       "      <td>14.25</td>\n",
       "      <td>4.44</td>\n",
       "      <td>walking</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33115</th>\n",
       "      <td>119000</td>\n",
       "      <td>27.33</td>\n",
       "      <td>1.25</td>\n",
       "      <td>11.33</td>\n",
       "      <td>0.94</td>\n",
       "      <td>20.00</td>\n",
       "      <td>4.00</td>\n",
       "      <td>walking</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33116</th>\n",
       "      <td>119250</td>\n",
       "      <td>37.80</td>\n",
       "      <td>7.68</td>\n",
       "      <td>14.20</td>\n",
       "      <td>2.48</td>\n",
       "      <td>17.25</td>\n",
       "      <td>0.83</td>\n",
       "      <td>walking</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33117</th>\n",
       "      <td>119500</td>\n",
       "      <td>33.75</td>\n",
       "      <td>1.30</td>\n",
       "      <td>15.75</td>\n",
       "      <td>5.21</td>\n",
       "      <td>16.50</td>\n",
       "      <td>2.69</td>\n",
       "      <td>walking</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33118</th>\n",
       "      <td>119750</td>\n",
       "      <td>32.67</td>\n",
       "      <td>3.09</td>\n",
       "      <td>18.67</td>\n",
       "      <td>0.47</td>\n",
       "      <td>14.00</td>\n",
       "      <td>3.16</td>\n",
       "      <td>walking</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>33119 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         time  avg_rss12  var_rss12  avg_rss13  var_rss13  avg_rss23  \\\n",
       "0           0      42.00       0.00      18.50       0.50      12.00   \n",
       "1         250      42.00       0.00      18.00       0.00      11.33   \n",
       "2         500      42.75       0.43      16.75       1.79      18.25   \n",
       "3         750      42.50       0.50      16.75       0.83      19.00   \n",
       "4        1000      43.00       0.82      16.25       0.83      18.00   \n",
       "...       ...        ...        ...        ...        ...        ...   \n",
       "33114  118750      31.50       1.66      12.50       3.20      14.25   \n",
       "33115  119000      27.33       1.25      11.33       0.94      20.00   \n",
       "33116  119250      37.80       7.68      14.20       2.48      17.25   \n",
       "33117  119500      33.75       1.30      15.75       5.21      16.50   \n",
       "33118  119750      32.67       3.09      18.67       0.47      14.00   \n",
       "\n",
       "       var_rss23    target dataset_idx  \n",
       "0           0.00  bending1           7  \n",
       "1           0.94  bending1           7  \n",
       "2           0.43  bending1           7  \n",
       "3           1.22  bending1           7  \n",
       "4           0.00  bending1           7  \n",
       "...          ...       ...         ...  \n",
       "33114       4.44   walking           9  \n",
       "33115       4.00   walking           9  \n",
       "33116       0.83   walking           9  \n",
       "33117       2.69   walking           9  \n",
       "33118       3.16   walking           9  \n",
       "\n",
       "[33119 rows x 9 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('------------------------------- TRAINING DATA -------------------------------')\n",
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b020741a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------- TESTING DATA -------------------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time</th>\n",
       "      <th>avg_rss12</th>\n",
       "      <th>var_rss12</th>\n",
       "      <th>avg_rss13</th>\n",
       "      <th>var_rss13</th>\n",
       "      <th>avg_rss23</th>\n",
       "      <th>var_rss23</th>\n",
       "      <th>target</th>\n",
       "      <th>dataset_idx</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>39.25</td>\n",
       "      <td>0.43</td>\n",
       "      <td>22.75</td>\n",
       "      <td>0.43</td>\n",
       "      <td>33.75</td>\n",
       "      <td>1.30</td>\n",
       "      <td>bending1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>250</td>\n",
       "      <td>39.25</td>\n",
       "      <td>0.43</td>\n",
       "      <td>23.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>33.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>bending1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>500</td>\n",
       "      <td>39.25</td>\n",
       "      <td>0.43</td>\n",
       "      <td>23.25</td>\n",
       "      <td>0.43</td>\n",
       "      <td>33.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>bending1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>750</td>\n",
       "      <td>39.50</td>\n",
       "      <td>0.50</td>\n",
       "      <td>23.00</td>\n",
       "      <td>0.71</td>\n",
       "      <td>33.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>bending1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1000</td>\n",
       "      <td>39.50</td>\n",
       "      <td>0.50</td>\n",
       "      <td>24.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>33.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>bending1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9115</th>\n",
       "      <td>118750</td>\n",
       "      <td>36.00</td>\n",
       "      <td>2.45</td>\n",
       "      <td>17.00</td>\n",
       "      <td>5.10</td>\n",
       "      <td>20.50</td>\n",
       "      <td>0.87</td>\n",
       "      <td>walking</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9116</th>\n",
       "      <td>119000</td>\n",
       "      <td>34.33</td>\n",
       "      <td>1.89</td>\n",
       "      <td>15.00</td>\n",
       "      <td>2.45</td>\n",
       "      <td>17.00</td>\n",
       "      <td>2.12</td>\n",
       "      <td>walking</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9117</th>\n",
       "      <td>119250</td>\n",
       "      <td>33.00</td>\n",
       "      <td>7.35</td>\n",
       "      <td>14.60</td>\n",
       "      <td>3.14</td>\n",
       "      <td>13.00</td>\n",
       "      <td>5.70</td>\n",
       "      <td>walking</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9118</th>\n",
       "      <td>119500</td>\n",
       "      <td>31.67</td>\n",
       "      <td>1.25</td>\n",
       "      <td>11.00</td>\n",
       "      <td>6.16</td>\n",
       "      <td>19.25</td>\n",
       "      <td>2.17</td>\n",
       "      <td>walking</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9119</th>\n",
       "      <td>119750</td>\n",
       "      <td>30.75</td>\n",
       "      <td>10.21</td>\n",
       "      <td>11.75</td>\n",
       "      <td>1.09</td>\n",
       "      <td>18.50</td>\n",
       "      <td>3.20</td>\n",
       "      <td>walking</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9120 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        time  avg_rss12  var_rss12  avg_rss13  var_rss13  avg_rss23  \\\n",
       "0          0      39.25       0.43      22.75       0.43      33.75   \n",
       "1        250      39.25       0.43      23.00       0.00      33.00   \n",
       "2        500      39.25       0.43      23.25       0.43      33.00   \n",
       "3        750      39.50       0.50      23.00       0.71      33.00   \n",
       "4       1000      39.50       0.50      24.00       0.00      33.00   \n",
       "...      ...        ...        ...        ...        ...        ...   \n",
       "9115  118750      36.00       2.45      17.00       5.10      20.50   \n",
       "9116  119000      34.33       1.89      15.00       2.45      17.00   \n",
       "9117  119250      33.00       7.35      14.60       3.14      13.00   \n",
       "9118  119500      31.67       1.25      11.00       6.16      19.25   \n",
       "9119  119750      30.75      10.21      11.75       1.09      18.50   \n",
       "\n",
       "      var_rss23    target dataset_idx  \n",
       "0          1.30  bending1           1  \n",
       "1          0.00  bending1           1  \n",
       "2          0.00  bending1           1  \n",
       "3          0.00  bending1           1  \n",
       "4          0.00  bending1           1  \n",
       "...         ...       ...         ...  \n",
       "9115       0.87   walking           3  \n",
       "9116       2.12   walking           3  \n",
       "9117       5.70   walking           3  \n",
       "9118       2.17   walking           3  \n",
       "9119       3.20   walking           3  \n",
       "\n",
       "[9120 rows x 9 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('------------------------------- TESTING DATA -------------------------------')\n",
    "test_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7235a181",
   "metadata": {},
   "source": [
    "###### c) i. Research what types of time-domain features are usually used in time series classification and list them (examples are minimum, maximum, mean, etc)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcec8aa9",
   "metadata": {},
   "source": [
    "- Standard Deviation\n",
    "- Variance\n",
    "- Skewness and Kutosis\n",
    "- Interquantile Range\n",
    "- Pairwise correlation\n",
    "- Mean and zero crossing rate\n",
    "- Minimum\n",
    "- Maximum\n",
    "- Mean\n",
    "- Median\n",
    "\n",
    "[Reference](https://stats.stackexchange.com/questions/50807/features-for-time-series-classification)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "866de37b",
   "metadata": {},
   "source": [
    "###### c) ii. Extract the time-domain features minimum, maximum, mean, median, standard deviation, first quartile, and third quartile for all of the 6 time series in each instance. You are free to normalize/standardize features or use them directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "437d194f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------------+----------------------+---------------+---------------+\n",
      "|    | Feature      |   Standard Deviation |   Lower Bound |   Upper Bound |\n",
      "|----+--------------+----------------------+---------------+---------------|\n",
      "|  0 | min_1        |            9.56998   |      8.33595  |    10.8135    |\n",
      "|  1 | max_1        |            4.39436   |      3.46166  |     5.41622   |\n",
      "|  2 | mean_1       |            5.33572   |      4.76991  |     5.92327   |\n",
      "|  3 | median_1     |            5.44005   |      4.85709  |     6.05814   |\n",
      "|  4 | std_1        |            1.77215   |      1.58089  |     1.96393   |\n",
      "|  5 | quantile_1_1 |            6.15359   |      5.62558  |     6.70259   |\n",
      "|  6 | quantile_3_1 |            5.13892   |      4.40391  |     5.908     |\n",
      "|  7 | min_2        |            0         |      0        |     0         |\n",
      "|  8 | max_2        |            5.06273   |      4.69325  |     5.47209   |\n",
      "|  9 | mean_2       |            1.57416   |      1.43267  |     1.74113   |\n",
      "| 10 | median_2     |            1.41224   |      1.27289  |     1.57677   |\n",
      "| 11 | std_2        |            0.884105  |      0.821513 |     0.95871   |\n",
      "| 12 | quantile_1_2 |            0.946386  |      0.852277 |     1.05662   |\n",
      "| 13 | quantile_3_2 |            2.12527   |      1.94286  |     2.34124   |\n",
      "| 14 | min_3        |            2.95646   |      2.78687  |     3.13827   |\n",
      "| 15 | max_3        |            4.87514   |      4.26563  |     5.54192   |\n",
      "| 16 | mean_3       |            4.00838   |      3.50416  |     4.57363   |\n",
      "| 17 | median_3     |            4.0364    |      3.51712  |     4.62065   |\n",
      "| 18 | std_3        |            0.94671   |      0.765408 |     1.12391   |\n",
      "| 19 | quantile_1_3 |            4.22066   |      3.71646  |     4.78348   |\n",
      "| 20 | quantile_3_3 |            4.17163   |      3.64357  |     4.77248   |\n",
      "| 21 | min_4        |            0         |      0        |     0         |\n",
      "| 22 | max_4        |            2.18363   |      1.99274  |     2.37627   |\n",
      "| 23 | mean_4       |            1.16611   |      1.10322  |     1.24817   |\n",
      "| 24 | median_4     |            1.14559   |      1.08273  |     1.22788   |\n",
      "| 25 | std_4        |            0.458242  |      0.427953 |     0.492632  |\n",
      "| 26 | quantile_1_4 |            0.84362   |      0.791273 |     0.907318  |\n",
      "| 27 | quantile_3_4 |            1.5525    |      1.46963  |     1.66202   |\n",
      "| 28 | min_5        |            6.124     |      4.67315  |     7.74645   |\n",
      "| 29 | max_5        |            5.74124   |      4.87976  |     6.68553   |\n",
      "| 30 | mean_5       |            5.67559   |      4.5727   |     6.85253   |\n",
      "| 31 | median_5     |            5.81378   |      4.66805  |     7.04861   |\n",
      "| 32 | std_5        |            1.0249    |      0.825912 |     1.23129   |\n",
      "| 33 | quantile_1_5 |            6.09647   |      4.939    |     7.32865   |\n",
      "| 34 | quantile_3_5 |            5.53172   |      4.49145  |     6.64509   |\n",
      "| 35 | min_6        |            0.0458382 |      0.013125 |     0.0911539 |\n",
      "| 36 | max_6        |            2.51892   |      2.26253  |     2.77123   |\n",
      "| 37 | mean_6       |            1.15481   |      1.08797  |     1.23812   |\n",
      "| 38 | median_6     |            1.08647   |      1.01897  |     1.17039   |\n",
      "| 39 | std_6        |            0.517617  |      0.486944 |     0.551996  |\n",
      "| 40 | quantile_1_6 |            0.758584  |      0.704691 |     0.822398  |\n",
      "| 41 | quantile_3_6 |            1.5236    |      1.43744  |     1.63234   |\n",
      "+----+--------------+----------------------+---------------+---------------+\n"
     ]
    }
   ],
   "source": [
    "time_series_data = pd.concat([train_data, test_data])\n",
    "\n",
    "def get_time_series_stats(stat):\n",
    "    cols = {\n",
    "        \"avg_rss12\": f'{stat}_1',\n",
    "        \"var_rss12\": f'{stat}_2',\n",
    "        \"avg_rss13\": f'{stat}_3', \n",
    "        \"var_rss13\": f'{stat}_4',\n",
    "        \"avg_rss23\": f'{stat}_5',\n",
    "        \"var_rss23\": f'{stat}_6'\n",
    "    }\n",
    "    \n",
    "    df = time_series_data.groupby([\"target\", \"dataset_idx\"], as_index=False)\n",
    "\n",
    "    if stat == 'min': values = df.min()\n",
    "    elif stat == 'max': values = df.max()\n",
    "    elif stat == 'mean': values = df.mean(numeric_only=True)\n",
    "    elif stat == 'median': values = df.median(numeric_only=True)\n",
    "    elif stat == 'std': values = df.std(numeric_only=True)\n",
    "    elif stat == 'quantile_1': values = df.quantile(0.25, numeric_only=True)\n",
    "    elif stat == 'quantile_3': values = df.quantile(0.75, numeric_only=True)\n",
    "    \n",
    "    values.rename(columns=cols, inplace = True)\n",
    "    return values\n",
    "\n",
    "statistics = ['min','max','mean','median','std','quantile_1', 'quantile_3']\n",
    "\n",
    "features = [f'{s}_{i}' for i in range(1, 7) for s in statistics]\n",
    "\n",
    "ans = []\n",
    "\n",
    "for s in statistics:\n",
    "    ans.append(get_time_series_stats(s))\n",
    "\n",
    "result = pd.concat(ans, axis = 1)\n",
    "\n",
    "result = result[features]\n",
    "\n",
    "print(tabulate(std_dev, headers='keys', tablefmt='psql'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1956562e",
   "metadata": {},
   "source": [
    "###### c iii) Estimate the standard deviation of each of the time-domain features you extracted from the data. Then, use Python’s bootstrapped or any other method to build a 90% bootsrap confidence interval for the standard deviation of each feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ab1185ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------------+----------------------+---------------+---------------+\n",
      "|    | Feature      |   Standard Deviation |   Lower Bound |   Upper Bound |\n",
      "|----+--------------+----------------------+---------------+---------------|\n",
      "|  0 | min_1        |            9.56998   |      8.33595  |    10.8135    |\n",
      "|  1 | max_1        |            4.39436   |      3.46166  |     5.41622   |\n",
      "|  2 | mean_1       |            5.33572   |      4.76991  |     5.92327   |\n",
      "|  3 | median_1     |            5.44005   |      4.85709  |     6.05814   |\n",
      "|  4 | std_1        |            1.77215   |      1.58089  |     1.96393   |\n",
      "|  5 | quantile_1_1 |            6.15359   |      5.62558  |     6.70259   |\n",
      "|  6 | quantile_3_1 |            5.13892   |      4.40391  |     5.908     |\n",
      "|  7 | min_2        |            0         |      0        |     0         |\n",
      "|  8 | max_2        |            5.06273   |      4.69325  |     5.47209   |\n",
      "|  9 | mean_2       |            1.57416   |      1.43267  |     1.74113   |\n",
      "| 10 | median_2     |            1.41224   |      1.27289  |     1.57677   |\n",
      "| 11 | std_2        |            0.884105  |      0.821513 |     0.95871   |\n",
      "| 12 | quantile_1_2 |            0.946386  |      0.852277 |     1.05662   |\n",
      "| 13 | quantile_3_2 |            2.12527   |      1.94286  |     2.34124   |\n",
      "| 14 | min_3        |            2.95646   |      2.78687  |     3.13827   |\n",
      "| 15 | max_3        |            4.87514   |      4.26563  |     5.54192   |\n",
      "| 16 | mean_3       |            4.00838   |      3.50416  |     4.57363   |\n",
      "| 17 | median_3     |            4.0364    |      3.51712  |     4.62065   |\n",
      "| 18 | std_3        |            0.94671   |      0.765408 |     1.12391   |\n",
      "| 19 | quantile_1_3 |            4.22066   |      3.71646  |     4.78348   |\n",
      "| 20 | quantile_3_3 |            4.17163   |      3.64357  |     4.77248   |\n",
      "| 21 | min_4        |            0         |      0        |     0         |\n",
      "| 22 | max_4        |            2.18363   |      1.99274  |     2.37627   |\n",
      "| 23 | mean_4       |            1.16611   |      1.10322  |     1.24817   |\n",
      "| 24 | median_4     |            1.14559   |      1.08273  |     1.22788   |\n",
      "| 25 | std_4        |            0.458242  |      0.427953 |     0.492632  |\n",
      "| 26 | quantile_1_4 |            0.84362   |      0.791273 |     0.907318  |\n",
      "| 27 | quantile_3_4 |            1.5525    |      1.46963  |     1.66202   |\n",
      "| 28 | min_5        |            6.124     |      4.67315  |     7.74645   |\n",
      "| 29 | max_5        |            5.74124   |      4.87976  |     6.68553   |\n",
      "| 30 | mean_5       |            5.67559   |      4.5727   |     6.85253   |\n",
      "| 31 | median_5     |            5.81378   |      4.66805  |     7.04861   |\n",
      "| 32 | std_5        |            1.0249    |      0.825912 |     1.23129   |\n",
      "| 33 | quantile_1_5 |            6.09647   |      4.939    |     7.32865   |\n",
      "| 34 | quantile_3_5 |            5.53172   |      4.49145  |     6.64509   |\n",
      "| 35 | min_6        |            0.0458382 |      0.013125 |     0.0911539 |\n",
      "| 36 | max_6        |            2.51892   |      2.26253  |     2.77123   |\n",
      "| 37 | mean_6       |            1.15481   |      1.08797  |     1.23812   |\n",
      "| 38 | median_6     |            1.08647   |      1.01897  |     1.17039   |\n",
      "| 39 | std_6        |            0.517617  |      0.486944 |     0.551996  |\n",
      "| 40 | quantile_1_6 |            0.758584  |      0.704691 |     0.822398  |\n",
      "| 41 | quantile_3_6 |            1.5236    |      1.43744  |     1.63234   |\n",
      "+----+--------------+----------------------+---------------+---------------+\n"
     ]
    }
   ],
   "source": [
    "cols = [\"Feature\", \"Standard Deviation\"]\n",
    "\n",
    "sd = result.describe().loc[['std']].values[0].tolist()\n",
    "\n",
    "std_dev = pd.DataFrame(list(zip(result.columns, sd)), columns = cols)\n",
    "\n",
    "def get_bound(type):\n",
    "    b = map(lambda x: bootstrap((np.array(result[[x]]),), np.std, random_state=42, confidence_level=0.9, method = 'basic').confidence_interval[0 if type == 'lower' else 1], result.columns)\n",
    "    return list(np.array(list(b)).flat)\n",
    "\n",
    "std_dev[\"Lower Bound\"] = get_bound('lower')\n",
    "\n",
    "std_dev[\"Upper Bound\"] = get_bound('upper')\n",
    "\n",
    "print(tabulate(std_dev, headers='keys', tablefmt='psql'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57212656",
   "metadata": {},
   "source": [
    "###### iv) Use your judgement to select the three most important time-domain features (one option may be min, mean, and max).\n",
    "\n",
    "- Mean: It gives us an idea of where the center value is located in a dataset.\n",
    "\n",
    "- Standard Deviation: It is a measure of how dispersed the data is in relation to the mean. Low standard deviation means data are clustered around the mean, and high standard deviation indicates data are more spread out.\n",
    "\n",
    "- Median:  It provides a helpful measure for the centre of a dataset. By comparing the median to the mean, you can get an idea of the distribution of a dataset. When the mean and the median are the same, the dataset is more or less evenly distributed from the lowest to highest values\n",
    "\n",
    "\n",
    "###### Note:\n",
    "Another feature we can use is Moving Average\n",
    "\n",
    "Moving Average - A moving average is defined as an average of fixed number of items in the time series which move through the series by dropping the top items of the previous averaged group and adding the next in each successive average.\n",
    "\n",
    "1. The moving average method eliminates the short-term fluctuations.\n",
    "2. It reduces the effect of extreme values\n",
    "\n",
    "\n",
    "[Reference 1](https://www.abs.gov.au/ausstats/abs@.nsf/products/6FA51527EA94B3CCCA25747400158EB5?opendocument)\n",
    "[Reference 2](https://www.sciencedirect.com/topics/engineering/moving-average#:~:text=A%20moving%20average%20is%20defined,next%20in%20each%20successive%20average.)\n",
    "[Reference 3](https://www.nlm.nih.gov/nichsr/stats_tutorial/section2/mod8_sd.html#:~:text=A%20standard%20deviation%20(or%20%CF%83,data%20are%20more%20spread%20out.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69fe8244",
   "metadata": {},
   "source": [
    "#### 2. I collect a set of data (n = 100 observations) containing a single predictor and a quantitative response. I then fit a linear regression model to the data, as well as a separate cubic regression, i.e. Y = β0 +β1X +β2X2 +β3X3 +ε."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3841ecd8",
   "metadata": {},
   "source": [
    "###### (a) Suppose that the true relationship between X and Y is linear, i.e. Y = β0 + β1X + ε. Consider the training residual sum of squares (RSS) for the linear regression, and also the training RSS for the cubic regression. Would we expect one to be lower than the other, would we expect them to be the same, or is there not enough information to tell? Justify your answer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ada76751",
   "metadata": {},
   "source": [
    "Ans. The cubic model is more flexible and will fit better to the training data than the linear model and so it will have a lower RSS as compared to the linear model on training data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "705c6be1",
   "metadata": {},
   "source": [
    "###### (b) Answer (a) using test rather than training RSS."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d87a93a",
   "metadata": {},
   "source": [
    "Ans. Knowing that the relationship between X and Y is linear we can expect the RSS for the linear model to be lower than the cubic model for testing data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67d97c13",
   "metadata": {},
   "source": [
    "###### c) Suppose that the true relationship between X and Y is not linear, but we don’t know how far it is from linear. Consider the training RSS for the linear regression, and also the training RSS for the cubic regression. Would we expect one to be lower than the other, would we expect them to be the same, or is there not enough information to tell? Justify your answer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eb4ed81",
   "metadata": {},
   "source": [
    "Ans. The cubic model is more flexible so I would expect it fit better to the training data and thus have a lower RSS on the training set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c05ca1d4",
   "metadata": {},
   "source": [
    "###### (d) Answer (c) using test rather than training RSS."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a515f655",
   "metadata": {},
   "source": [
    "Ans. There is not enough information in this case; and the answer depends on how non-linear the true relationship is. \n",
    "\n",
    "- In the case that the true relationship is closer to linear than cubic the linear model will perform better. \n",
    "- In case the true relationship is closer to a cubic relationship the cubic model will perform better."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
